{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples : 76700\n",
      "Training samples : 61360\n",
      "Test samples : 15340\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11953a630>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': None}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1000\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.00263071, step = 1001\n",
      "INFO:tensorflow:Saving checkpoints for 1100 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00096414.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:17:09\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1100\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:17:09\n",
      "INFO:tensorflow:Saving dict for global step 1100: Train accuracy = 0.96, global_step = 1100, loss = 0.00557695\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:17:12\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1100\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:17:13\n",
      "INFO:tensorflow:Saving dict for global step 1100: Val accuracy = 0.97, global_step = 1100, loss = 0.0389241\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1100\n",
      "INFO:tensorflow:Saving checkpoints for 1101 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0056842, step = 1101\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00142685.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:18:14\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1200\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:18:14\n",
      "INFO:tensorflow:Saving dict for global step 1200: Train accuracy = 0.97, global_step = 1200, loss = 0.00137536\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:18:15\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1200\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:18:16\n",
      "INFO:tensorflow:Saving dict for global step 1200: Val accuracy = 0.98, global_step = 1200, loss = 0.00249993\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1200\n",
      "INFO:tensorflow:Saving checkpoints for 1201 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.00500644, step = 1201\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0013701.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:19:17\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1300\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:19:18\n",
      "INFO:tensorflow:Saving dict for global step 1300: Train accuracy = 0.95, global_step = 1300, loss = 0.00606208\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:19:19\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1300\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:19:20\n",
      "INFO:tensorflow:Saving dict for global step 1300: Val accuracy = 0.96, global_step = 1300, loss = 0.00622423\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1300\n",
      "INFO:tensorflow:Saving checkpoints for 1301 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.00586351, step = 1301\n",
      "INFO:tensorflow:Saving checkpoints for 1400 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00104557.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:20:22\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1400\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:20:23\n",
      "INFO:tensorflow:Saving dict for global step 1400: Train accuracy = 0.99, global_step = 1400, loss = 0.00326714\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:20:24\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1400\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:20:25\n",
      "INFO:tensorflow:Saving dict for global step 1400: Val accuracy = 0.98, global_step = 1400, loss = 0.00338687\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1400\n",
      "INFO:tensorflow:Saving checkpoints for 1401 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.00424549, step = 1401\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0011084.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:21:29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1500\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:21:30\n",
      "INFO:tensorflow:Saving dict for global step 1500: Train accuracy = 0.99, global_step = 1500, loss = 0.00221696\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:21:31\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1500\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:21:31\n",
      "INFO:tensorflow:Saving dict for global step 1500: Val accuracy = 0.96, global_step = 1500, loss = 0.00481582\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1500\n",
      "INFO:tensorflow:Saving checkpoints for 1501 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.00271456, step = 1501\n",
      "INFO:tensorflow:Saving checkpoints for 1600 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.000733422.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:22:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1600\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:22:35\n",
      "INFO:tensorflow:Saving dict for global step 1600: Train accuracy = 0.95, global_step = 1600, loss = 0.00340402\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:22:36\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1600\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:22:37\n",
      "INFO:tensorflow:Saving dict for global step 1600: Val accuracy = 0.98, global_step = 1600, loss = 0.00395212\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1600\n",
      "INFO:tensorflow:Saving checkpoints for 1601 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.00250621, step = 1601\n",
      "INFO:tensorflow:Saving checkpoints for 1700 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00106296.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:23:38\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1700\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:23:39\n",
      "INFO:tensorflow:Saving dict for global step 1700: Train accuracy = 0.99, global_step = 1700, loss = 0.00146324\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:23:40\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1700\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:23:41\n",
      "INFO:tensorflow:Saving dict for global step 1700: Val accuracy = 0.96, global_step = 1700, loss = 0.00308506\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1700\n",
      "INFO:tensorflow:Saving checkpoints for 1701 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0021746, step = 1701\n",
      "INFO:tensorflow:Saving checkpoints for 1800 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00114209.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:24:43\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1800\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:24:43\n",
      "INFO:tensorflow:Saving dict for global step 1800: Train accuracy = 0.97, global_step = 1800, loss = 0.00121581\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:24:44\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1800\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:24:45\n",
      "INFO:tensorflow:Saving dict for global step 1800: Val accuracy = 0.96, global_step = 1800, loss = 0.00287492\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1800\n",
      "INFO:tensorflow:Saving checkpoints for 1801 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.00259366, step = 1801\n",
      "INFO:tensorflow:Saving checkpoints for 1900 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0014339.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:25:51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1900\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:25:51\n",
      "INFO:tensorflow:Saving dict for global step 1900: Train accuracy = 0.96, global_step = 1900, loss = 0.00320922\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:25:53\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1900\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:25:53\n",
      "INFO:tensorflow:Saving dict for global step 1900: Val accuracy = 0.98, global_step = 1900, loss = 0.00260401\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-1900\n",
      "INFO:tensorflow:Saving checkpoints for 1901 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.00357301, step = 1901\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/cnn_prob/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00117724.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:27:13\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-2000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:27:14\n",
      "INFO:tensorflow:Saving dict for global step 2000: Train accuracy = 1.0, global_step = 2000, loss = 0.00199641\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:27:15\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-2000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:27:16\n",
      "INFO:tensorflow:Saving dict for global step 2000: Val accuracy = 0.94, global_step = 2000, loss = 0.00682776\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-01-18:31:35\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cnn_prob/model.ckpt-2000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-18:36:25\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.970078, global_step = 2000, loss = 0.00344711\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "Training done in 1220.1621220111847 seconds.\n"
     ]
    }
   ],
   "source": [
    "# learning using a CNN\n",
    "\n",
    "# CNN for learning!\n",
    "\n",
    "# learn the states of a double dot\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# application logic will be added here\n",
    "def cnn_model_fn(features,labels,mode):\n",
    "    '''Model function for CNN'''\n",
    "    #input layer\n",
    "    input_layer = tf.cast(tf.reshape(features,[-1,30,30,1]),tf.float32)\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer,\n",
    "                            filters=32,\n",
    "                            kernel_size=[5,5],\n",
    "                            padding=\"same\",\n",
    "                            activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2],strides=2)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                            filters=64,\n",
    "                            kernel_size=[5,5],\n",
    "                            padding=\"same\",\n",
    "                            activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2],strides=2)\n",
    "    \n",
    "    flat = tf.contrib.layers.flatten(inputs=pool2)\n",
    "    # dense output layer\n",
    "    out1 = tf.layers.dense(inputs=flat,units=1024,activation=tf.nn.relu)  \n",
    "    dropout1 = tf.layers.dropout(\n",
    "      inputs=out1, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\n",
    "    \n",
    "    out = tf.layers.dense(inputs=dropout1, units=4)\n",
    "    \n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "    # Calculate loss( for both TRAIN AND EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        loss = tf.losses.mean_squared_error(out,labels['prob'])\n",
    "\n",
    "    # Configure the training op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=1e-3,\n",
    "            optimizer=tf.train.AdamOptimizer)\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions= {\n",
    "        \"prob\" : out,\n",
    "        \"states\" : tf.argmax(out,axis=1),\n",
    "    }\n",
    "    \n",
    "    # Returna  ModelFnOps object\n",
    "    return model_fn_lib.ModelFnOps(mode=mode,predictions=predictions,loss=loss, train_op=train_op)\n",
    "    \n",
    "def get_train_inputs():\n",
    "    n_batch = 100\n",
    "    index = np.random.choice(n_train,n_batch,replace=False)\n",
    "    inp = []\n",
    "    oup = []\n",
    "    for i in index:\n",
    "        dat = np.load(files[i])\n",
    "        inp += [dat.item()['current_map']]\n",
    "        oup += [dat.item()['label']]\n",
    "\n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    oup = np.array(oup,dtype=np.float32)\n",
    "    \n",
    "    x = tf.constant(inp)\n",
    "    y = tf.constant(oup)\n",
    "    \n",
    "    labels_dict = {}\n",
    "    labels_dict['prob'] = y\n",
    "    labels_dict['states'] = tf.argmax(y,axis=1)\n",
    "    return x,labels_dict\n",
    "\n",
    "def get_val_inputs():\n",
    "    inp = []\n",
    "    oup = []\n",
    "       \n",
    "    for file in files[n_train:(n_train + 100)]:\n",
    "        dat = np.load(file)\n",
    "        inp += [dat.item()['current_map']]\n",
    "        oup += [dat.item()['label']]\n",
    "    \n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    oup = np.array(oup,dtype=np.float32)\n",
    "    \n",
    "    x = tf.constant(inp)\n",
    "    y = tf.constant(oup)\n",
    "    \n",
    "    labels_dict = {}\n",
    "    labels_dict['prob'] = y\n",
    "    labels_dict['states'] = tf.argmax(y,axis=1)\n",
    "    return x,labels_dict\n",
    "\n",
    "def get_test_inputs():\n",
    "    inp = []\n",
    "    oup = []\n",
    "       \n",
    "    for file in files[n_train:]:\n",
    "        dat = np.load(file)\n",
    "        inp += [dat.item()['current_map']]\n",
    "        oup += [dat.item()['label']]\n",
    "    \n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    oup = np.array(oup,dtype=np.float32)\n",
    "    \n",
    "    x = tf.constant(inp)\n",
    "    y = tf.constant(oup)\n",
    "    \n",
    "    labels_dict = {}\n",
    "    labels_dict['prob'] = y\n",
    "    labels_dict['states'] = tf.argmax(y,axis=1)\n",
    "    return x,labels_dict\n",
    "\n",
    "# get the data\n",
    "data_folder_path = \"/Users/ssk4/Downloads/data_subimage/\"\n",
    "files = glob.glob(data_folder_path + \"*.npy\")\n",
    "\n",
    "# shuffling the files to avoid any single dot bias\n",
    "import random\n",
    "random.shuffle(files)\n",
    "files = files[:]\n",
    "\n",
    "n_samples = len(files)\n",
    "train_sample_ratio = 0.8\n",
    "n_train = int(train_sample_ratio * n_samples)\n",
    "\n",
    "print(\"Total number of samples :\",n_samples)\n",
    "print(\"Training samples :\",n_train)\n",
    "print(\"Test samples :\",n_samples - n_train)\n",
    "\n",
    "st = time.time()\n",
    "# create the estimator\n",
    "dd_classifier = learn.Estimator(model_fn=cnn_model_fn,model_dir=\"/tmp/cnn_prob/\")\n",
    "\n",
    "n_epochs = 10\n",
    "steps_per_epoch = 100\n",
    "for _ in range(n_epochs):\n",
    "    dd_classifier.fit(\n",
    "        input_fn=get_train_inputs,\n",
    "        steps=steps_per_epoch)\n",
    "\n",
    "    train_metrics = {\n",
    "        \"Train accuracy\" : learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"states\",label_key=\"states\"),\n",
    "    }\n",
    "    dd_classifier.evaluate(input_fn=get_train_inputs,metrics=train_metrics,steps=1)\n",
    "\n",
    "    val_metrics = {\n",
    "        \"Val accuracy\" : learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"states\",label_key=\"states\"),\n",
    "    }\n",
    "    dd_classifier.evaluate(input_fn=get_val_inputs,metrics=val_metrics,steps=1)\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\" : learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"states\",label_key=\"states\"),\n",
    "}\n",
    "dd_classifier.evaluate(input_fn=get_test_inputs,metrics=metrics,steps=1)\n",
    "print(\"Training done in\",time.time()-st,\"seconds.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-167-99311670a755>:22: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hs/nm7qn85519v78y5bbl49h61h001sq3/T/tmpnel54pgp/model.ckpt-5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def accr_test_input_fn():\n",
    "    inp = []\n",
    "    oup = []\n",
    "       \n",
    "    for file in files[n_train:(n_train + 100)]:\n",
    "        dat = np.load(file)\n",
    "        inp += [dat.item()['current_map']]\n",
    "        oup += [dat.item()['label']]\n",
    "    \n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    oup = np.array(oup,dtype=np.float32)\n",
    "    \n",
    "    x = tf.constant(inp)\n",
    "    y = tf.constant(oup)\n",
    "    return x,y\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\" : learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"states\"),\n",
    "}\n",
    "\n",
    "pred = dd_classifier.predict(input_fn=accr_test_input_fn,as_iterable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-07-01-17:55:59\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hs/nm7qn85519v78y5bbl49h61h001sq3/T/tmp7me_3mgw/model.ckpt-100\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-17:56:00\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.93, global_step = 100, loss = 0.0134305\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.93000001, 'global_step': 100, 'loss': 0.013430518}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"accuracy\" : learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"states\",label_key=\"states\"),\n",
    "}\n",
    "dd_classifier.evaluate(input_fn=get_train_inputs,metrics=metrics,steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3,\n",
       "       2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2,\n",
       "       2, 2, 3, 2, 2, 2, 3, 3])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.53586641e-03,   5.17740846e-03,   9.50941443e-01,\n",
       "          2.10614204e-02],\n",
       "       [  1.37775205e-03,   3.36754322e-03,   9.73362803e-01,\n",
       "          9.35178995e-03],\n",
       "       [  5.08314930e-04,  -4.10084426e-03,   7.75267899e-01,\n",
       "          2.43259087e-01],\n",
       "       [  9.66005959e-04,  -1.75612420e-03,   6.38581038e-01,\n",
       "          3.70760769e-01],\n",
       "       [  4.98790294e-04,   2.79003754e-03,   1.00814617e+00,\n",
       "         -4.23276424e-03],\n",
       "       [  8.87231901e-04,   8.87972862e-03,   2.14482144e-01,\n",
       "          7.55513906e-01],\n",
       "       [  8.70760530e-04,  -5.79632074e-03,   6.50480747e-01,\n",
       "          3.68960559e-01],\n",
       "       [  4.69686463e-04,   6.43382221e-03,   9.58739400e-01,\n",
       "          3.81074250e-02],\n",
       "       [  5.87210059e-04,   2.56353058e-02,   1.01916778e+00,\n",
       "         -2.43719220e-02],\n",
       "       [  3.05661000e-04,   2.05680728e-04,   3.28062057e-01,\n",
       "          6.84476078e-01],\n",
       "       [  2.39092670e-03,   1.54843740e-02,   8.87747407e-01,\n",
       "          5.56392074e-02],\n",
       "       [  2.58115120e-04,  -1.05108246e-02,   1.05984473e+00,\n",
       "         -1.71020031e-02],\n",
       "       [  7.47594051e-04,  -4.11675125e-03,   1.01621664e+00,\n",
       "         -2.63503194e-03],\n",
       "       [ -5.66071831e-04,   5.56235760e-03,   2.25614011e-02,\n",
       "          9.82613742e-01],\n",
       "       [  4.17692587e-04,   3.20246443e-03,   2.74223447e-01,\n",
       "          7.24481761e-01],\n",
       "       [  3.87709588e-06,   1.26091704e-01,   9.54131484e-01,\n",
       "         -3.64763737e-02],\n",
       "       [  5.67901693e-03,   1.02226205e-01,   9.67179298e-01,\n",
       "         -4.03436422e-02],\n",
       "       [  1.25361793e-03,   6.49347156e-03,   9.68709469e-01,\n",
       "          1.40236765e-02],\n",
       "       [  1.66207738e-03,   4.42840159e-03,   6.51361585e-01,\n",
       "          3.19475412e-01],\n",
       "       [  9.28110443e-04,   1.39953867e-02,   9.71236587e-01,\n",
       "          3.21336091e-03],\n",
       "       [  8.15638341e-04,   8.72313976e-04,   1.00527418e+00,\n",
       "          2.56505609e-03],\n",
       "       [ -3.06621194e-04,   1.02197900e-02,   3.20366323e-02,\n",
       "          9.60138202e-01],\n",
       "       [  2.26744171e-03,   4.32547182e-02,   6.00103855e-01,\n",
       "          3.56535316e-01],\n",
       "       [  1.46848336e-03,   2.23178528e-02,   9.64352489e-01,\n",
       "          9.38296318e-04],\n",
       "       [  1.34263746e-03,   1.36427954e-03,   9.55325365e-01,\n",
       "          2.65608281e-02],\n",
       "       [  1.64419785e-03,   9.45420563e-03,   9.63532090e-01,\n",
       "          1.35118663e-02],\n",
       "       [ -3.63837928e-04,  -8.17778707e-03,   1.04796076e+00,\n",
       "          1.25220269e-02],\n",
       "       [  2.86182761e-03,   1.72755197e-02,   7.27676630e-01,\n",
       "          1.90096512e-01],\n",
       "       [  8.24103132e-04,  -1.44651532e-03,   1.01151967e+00,\n",
       "         -3.01909447e-03],\n",
       "       [  1.16623482e-02,   8.04407001e-02,   8.02424312e-01,\n",
       "          5.67801893e-02],\n",
       "       [  1.11796446e-02,   2.89719820e-01,   8.02401662e-01,\n",
       "         -1.82116628e-02],\n",
       "       [  6.71632588e-04,   1.36267766e-03,   9.92137313e-01,\n",
       "          5.69683313e-03],\n",
       "       [ -1.83896162e-04,   3.15372273e-02,   1.00493324e+00,\n",
       "         -5.27173281e-03],\n",
       "       [  2.45934539e-03,   1.24983378e-02,   8.47004533e-01,\n",
       "          9.50867534e-02],\n",
       "       [ -2.38071196e-04,   1.44472897e-01,   8.95504594e-01,\n",
       "         -7.60203600e-03],\n",
       "       [  1.00167096e-03,   1.79855190e-02,   2.92317271e-01,\n",
       "          6.63602710e-01],\n",
       "       [  5.68266027e-04,  -1.72440708e-03,   4.34956253e-01,\n",
       "          5.76852620e-01],\n",
       "       [ -5.07383607e-04,   4.35791165e-03,   3.51708829e-02,\n",
       "          9.70925450e-01],\n",
       "       [  8.08550045e-04,  -6.92734867e-03,   9.43309426e-01,\n",
       "          8.00341964e-02],\n",
       "       [  8.17167573e-04,  -1.53177977e-03,   1.00094175e+00,\n",
       "          2.95725465e-03],\n",
       "       [ -3.59052792e-04,   8.41400027e-03,   2.91620195e-02,\n",
       "          9.66757655e-01],\n",
       "       [  1.01063494e-03,   4.03824449e-03,   9.69061017e-01,\n",
       "          1.39303952e-02],\n",
       "       [  6.59182668e-04,   4.76231426e-03,   9.96148348e-01,\n",
       "         -2.58946419e-03],\n",
       "       [  1.54439174e-03,   4.17821109e-03,   5.64432800e-01,\n",
       "          4.18140084e-01],\n",
       "       [  1.63041428e-03,   3.92793864e-03,   8.72108579e-01,\n",
       "          9.66561735e-02],\n",
       "       [  4.07937914e-04,  -2.67503411e-03,   7.89289117e-01,\n",
       "          2.33141661e-01],\n",
       "       [  1.24827586e-03,   1.78275257e-03,   9.70643878e-01,\n",
       "          1.12838149e-02],\n",
       "       [  8.47622752e-04,   2.92966515e-03,   9.58679199e-01,\n",
       "          2.51457095e-02],\n",
       "       [  2.52182875e-03,   1.18899792e-02,   8.31291556e-01,\n",
       "          9.44815576e-02],\n",
       "       [  7.08202831e-04,   8.26808065e-02,   9.37207699e-01,\n",
       "         -1.22717023e-03],\n",
       "       [ -6.54630363e-04,   4.77458909e-03,   1.62093341e-02,\n",
       "          9.92393732e-01],\n",
       "       [  9.25727189e-04,  -5.83702326e-03,   9.43114758e-01,\n",
       "          7.68313110e-02],\n",
       "       [  1.77377835e-04,  -1.93938613e-05,   6.74325645e-01,\n",
       "          3.46136212e-01],\n",
       "       [  7.21039250e-04,  -7.14695454e-03,   1.00439179e+00,\n",
       "          1.13523006e-02],\n",
       "       [  1.07571203e-03,   2.20552459e-03,   9.81594086e-01,\n",
       "          8.79433751e-03],\n",
       "       [  1.03669269e-02,   1.17477894e-01,   9.61061358e-01,\n",
       "         -3.01026106e-02],\n",
       "       [  3.97859141e-04,  -8.49153101e-03,   8.61365318e-01,\n",
       "          1.79599836e-01],\n",
       "       [ -5.23767062e-03,  -3.20372283e-02,   1.34997523e+00,\n",
       "         -1.09149337e-01],\n",
       "       [  7.23217893e-03,   1.61800981e-01,   9.02897954e-01,\n",
       "         -3.21997404e-02],\n",
       "       [  1.83595344e-04,  -1.30607635e-02,   1.08201849e+00,\n",
       "         -3.07914913e-02],\n",
       "       [  1.26403477e-03,   1.47341490e-02,   9.52536464e-01,\n",
       "          1.08629912e-02],\n",
       "       [  1.83034688e-04,   1.85946375e-03,   2.95656145e-01,\n",
       "          7.13636756e-01],\n",
       "       [ -3.20533291e-05,  -3.28697264e-03,   1.06046212e+00,\n",
       "         -2.46390998e-02],\n",
       "       [  1.62732601e-03,   7.20009208e-03,   5.19666493e-01,\n",
       "          4.55618709e-01],\n",
       "       [ -2.04388052e-05,   3.34842876e-03,   1.68870851e-01,\n",
       "          8.32732916e-01],\n",
       "       [  1.00294501e-03,   4.48520482e-03,   7.33243346e-01,\n",
       "          2.47217700e-01],\n",
       "       [  4.06623073e-03,   9.92218778e-03,   6.50843263e-01,\n",
       "          2.92543739e-01],\n",
       "       [  8.47009011e-04,  -2.74063647e-03,   1.01201582e+00,\n",
       "         -2.60227919e-03],\n",
       "       [ -5.79264015e-04,   4.55665588e-03,   2.12527215e-02,\n",
       "          9.85547960e-01],\n",
       "       [  3.16131860e-03,   1.49658844e-02,   4.97868210e-01,\n",
       "          4.19808507e-01],\n",
       "       [  2.49119103e-03,   1.36102661e-02,   7.95333624e-01,\n",
       "          1.04538321e-01],\n",
       "       [ -5.54055907e-04,   5.73243201e-03,   2.35709250e-02,\n",
       "          9.81115162e-01],\n",
       "       [  1.40285492e-03,  -1.43051147e-06,   8.40638518e-01,\n",
       "          1.48132652e-01],\n",
       "       [  2.33384222e-03,   1.88882276e-03,   6.79910898e-01,\n",
       "          2.96425372e-01],\n",
       "       [  1.26260892e-03,   6.04952872e-03,   6.13144159e-01,\n",
       "          3.69573206e-01],\n",
       "       [  8.98027793e-04,   2.28247792e-03,   6.01194859e-01,\n",
       "          3.95992905e-01],\n",
       "       [ -7.52207637e-03,   3.26109916e-01,   7.36782074e-01,\n",
       "         -1.23046935e-02],\n",
       "       [  1.58115663e-03,   4.40952592e-02,   5.99486649e-01,\n",
       "          3.05624723e-01],\n",
       "       [  7.64749013e-04,   2.11306289e-03,   9.66441870e-01,\n",
       "          3.26219201e-02],\n",
       "       [  1.63781270e-03,   1.02468207e-02,   5.94302654e-01,\n",
       "          3.65747094e-01],\n",
       "       [  8.90163705e-04,  -2.13201344e-03,   9.91318226e-01,\n",
       "          8.96710157e-03],\n",
       "       [  1.67580508e-03,   9.96024907e-03,   5.45021713e-01,\n",
       "          4.16310281e-01],\n",
       "       [  6.04310073e-04,   5.95795363e-03,   9.24239993e-01,\n",
       "          6.98844492e-02],\n",
       "       [ -6.25185668e-04,   2.94831395e-03,   7.38100708e-02,\n",
       "          9.40092266e-01],\n",
       "       [  1.12474617e-03,   8.64905864e-03,   9.17384505e-01,\n",
       "          5.88173866e-02],\n",
       "       [  2.42106989e-03,   2.10845806e-02,   8.15902233e-01,\n",
       "          7.98137486e-02],\n",
       "       [ -1.94998458e-04,   5.79770282e-03,   1.30781114e-01,\n",
       "          8.70872855e-01],\n",
       "       [  5.25278971e-04,  -5.10852784e-03,   1.01711786e+00,\n",
       "          2.37151980e-03],\n",
       "       [  8.82014632e-04,   6.35430217e-04,   4.19752061e-01,\n",
       "          5.80813169e-01],\n",
       "       [  1.24268234e-03,   4.47004475e-02,   8.95658612e-01,\n",
       "          4.41329032e-02],\n",
       "       [  1.35124931e-02,   2.19580419e-02,   9.09970284e-01,\n",
       "          1.43587649e-01],\n",
       "       [  9.44587402e-04,  -2.26093084e-03,   9.83043432e-01,\n",
       "          2.25315243e-02],\n",
       "       [  5.47110103e-04,  -3.70258093e-03,   1.02321124e+00,\n",
       "         -1.02322996e-02],\n",
       "       [  9.21153463e-04,   7.14763999e-03,   9.80939984e-01,\n",
       "          9.15244222e-04],\n",
       "       [ -5.19853085e-04,   3.88704985e-03,   9.76406336e-02,\n",
       "          9.14453030e-01],\n",
       "       [  8.80877860e-03,   3.46845061e-01,   6.71135008e-01,\n",
       "         -1.01506412e-02],\n",
       "       [  7.11125694e-03,   9.09093097e-02,   8.95801187e-01,\n",
       "          8.62199068e-03],\n",
       "       [  1.09204184e-03,   5.98566607e-03,   9.88742352e-01,\n",
       "         -2.36272812e-03],\n",
       "       [  3.83276492e-04,   3.76834720e-03,   2.39822790e-01,\n",
       "          7.54230857e-01],\n",
       "       [  9.77162272e-04,   7.61814043e-03,   2.61006474e-01,\n",
       "          7.14003205e-01]], dtype=float32)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.69222224  0.30777779]\n",
      " [ 0.          0.          0.50222224  0.49777779]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.18444444  0.81555557]\n",
      " [ 0.          0.          0.52888888  0.47111112]\n",
      " [ 0.          0.          0.94222224  0.05777778]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.24721603  0.75278395]\n",
      " [ 0.          0.          0.99333334  0.00666667]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.09111111  0.90888888]\n",
      " [ 0.          0.          0.61444443  0.38555557]\n",
      " [ 0.          0.36666667  0.63333333  0.        ]\n",
      " [ 0.01222222  0.10333333  0.88444442  0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.64333332  0.35666665]\n",
      " [ 0.          0.05555556  0.94444442  0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.01333333  0.05888889  0.92777777]\n",
      " [ 0.          0.07111111  0.55333334  0.37555555]\n",
      " [ 0.          0.00888889  0.98777777  0.00333333]\n",
      " [ 0.          0.          0.92444444  0.07555556]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.95999998  0.04      ]\n",
      " [ 0.          0.          0.93222225  0.06777778]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.06        0.10555556  0.83444446  0.        ]\n",
      " [ 0.          0.50777775  0.46888888  0.02333333]\n",
      " [ 0.          0.00333333  0.99666667  0.        ]\n",
      " [ 0.          0.00111111  0.99888891  0.        ]\n",
      " [ 0.          0.          0.99888891  0.00111111]\n",
      " [ 0.          0.14222223  0.85666668  0.00111111]\n",
      " [ 0.          0.02444444  0.27888888  0.69666666]\n",
      " [ 0.          0.          0.44333333  0.55666667]\n",
      " [ 0.          0.          0.16222222  0.83777779]\n",
      " [ 0.          0.          0.85888886  0.14111111]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.00888889  0.03555556  0.95555556]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.99888891  0.00111111]\n",
      " [ 0.          0.          0.40666667  0.5933333 ]\n",
      " [ 0.          0.          0.85111111  0.14888889]\n",
      " [ 0.          0.          0.8888889   0.11111111]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.76888889  0.23111111]\n",
      " [ 0.          0.          0.96777779  0.03222222]\n",
      " [ 0.          0.06555556  0.93444443  0.        ]\n",
      " [ 0.          0.          0.00666667  0.99333334]\n",
      " [ 0.          0.          0.90222222  0.09777778]\n",
      " [ 0.          0.          0.59111112  0.40888888]\n",
      " [ 0.          0.          0.92666668  0.07333333]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.41222224  0.51555556  0.07222223]\n",
      " [ 0.          0.          0.75777775  0.24222222]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.13666667  0.85777777  0.00555556]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.01666667  0.98333335  0.        ]\n",
      " [ 0.          0.          0.28999999  0.70999998]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.47        0.52999997]\n",
      " [ 0.          0.          0.20888889  0.79111111]\n",
      " [ 0.          0.          0.68777776  0.31222221]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.01444444  0.98555553]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.09222222  0.90777779]\n",
      " [ 0.          0.          0.83111113  0.16888888]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.47111112  0.52888888]\n",
      " [ 0.          0.          0.62111109  0.37888888]\n",
      " [ 0.05111111  0.27555555  0.67333335  0.        ]\n",
      " [ 0.          0.          0.93888891  0.06111111]\n",
      " [ 0.          0.          0.98888886  0.01111111]\n",
      " [ 0.          0.01333333  0.59222221  0.39444444]\n",
      " [ 0.          0.          0.89888889  0.10111111]\n",
      " [ 0.          0.          0.3611111   0.6388889 ]\n",
      " [ 0.          0.          0.93666667  0.06333333]\n",
      " [ 0.          0.          0.10444444  0.89555556]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.70666665  0.29333332]\n",
      " [ 0.          0.          0.06111111  0.93888891]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.39555556  0.60444444]\n",
      " [ 0.          0.04444445  0.95555556  0.        ]\n",
      " [ 0.02555555  0.04444445  0.93000001  0.        ]\n",
      " [ 0.          0.          0.95999998  0.04      ]\n",
      " [ 0.          0.00111111  0.94        0.05888889]\n",
      " [ 0.          0.00333333  0.99666667  0.        ]\n",
      " [ 0.          0.          0.06111111  0.93888891]\n",
      " [ 0.          0.45222223  0.54777777  0.        ]\n",
      " [ 0.03777778  0.10222222  0.86000001  0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.20999999  0.79000002]\n",
      " [ 0.          0.          0.2588889   0.7411111 ]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x,y = accr_test_input_fn()\n",
    "st = tf.argmax(y,axis=1)\n",
    "curr = sess.run(x)\n",
    "out = sess.run(y)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff = out - pred['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(diff).count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2]\n",
      "1 [3]\n",
      "2 [3]\n",
      "3 [3]\n",
      "4 [3]\n",
      "5 [3]\n",
      "6 [3]\n",
      "7 [2]\n",
      "8 [2]\n",
      "9 [3]\n",
      "10 [3]\n",
      "11 [2]\n",
      "12 [3]\n",
      "13 [3]\n",
      "14 [3]\n",
      "15 [2]\n",
      "16 [3]\n",
      "17 [3]\n",
      "18 [3]\n",
      "19 [3]\n",
      "20 [3]\n",
      "21 [3]\n",
      "22 [3]\n",
      "23 [3]\n",
      "24 [3]\n",
      "25 [3]\n",
      "26 [3]\n",
      "27 [3]\n",
      "28 [3]\n",
      "29 [3]\n",
      "30 [3]\n",
      "31 [3]\n",
      "32 [3]\n",
      "33 [3]\n",
      "34 [3]\n",
      "35 [3]\n",
      "36 [2]\n",
      "37 [2]\n",
      "38 [2]\n",
      "39 [3]\n",
      "40 [3]\n",
      "41 [3]\n",
      "42 [3]\n",
      "43 [3]\n",
      "44 [3]\n",
      "45 [3]\n",
      "46 [2]\n",
      "47 [2]\n",
      "48 [2]\n",
      "49 [2]\n",
      "50 [3]\n",
      "51 [2]\n",
      "52 [3]\n",
      "53 [3]\n"
     ]
    }
   ],
   "source": [
    "# Testing of the experimental data loading\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "\n",
    "data_folder_path = \"/Users/ssk4/Downloads/exp_data/\"\n",
    "files = glob.glob(data_folder_path + \"*.dat\")\n",
    "\n",
    "# Data format is V_LGD I_DC(nA) V_LGS I_AC(nA) t(sec)\n",
    "# The format of the loaded array is [num_points,5]\n",
    "index = np.random.randint(len(files))\n",
    "\n",
    "for i in range(len(files)):\n",
    "\n",
    "    dat = np.loadtxt(files[i])\n",
    "\n",
    "    sub_size = 30\n",
    "    grid_x = np.linspace(np.min(dat[:,0]),np.max(dat[:,0]),sub_size)\n",
    "    grid_y = np.linspace(np.min(dat[:,2]),np.max(dat[:,2]),sub_size)\n",
    "    xx,yy = np.meshgrid(grid_x,grid_y)\n",
    "    interpolated_data = scipy.interpolate.griddata((dat[:,0],dat[:,2]),dat[:,1],(xx, yy), method='nearest')\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #%matplotlib inline\n",
    "    #plt.pcolor(interpolated_data)\n",
    "\n",
    "    print(i,dd_classifier.predict(x=interpolated_data,as_iterable=False)['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x11e731c88>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e6fce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat = np.loadtxt(files[53])\n",
    "\n",
    "sub_size = 30\n",
    "grid_x = np.linspace(np.min(dat[:,0]),np.max(dat[:,0]),sub_size)\n",
    "grid_y = np.linspace(np.min(dat[:,2]),np.max(dat[:,2]),sub_size)\n",
    "xx,yy = np.meshgrid(grid_x,grid_y)\n",
    "interpolated_data = scipy.interpolate.griddata((dat[:,0],dat[:,2]),dat[:,1],(xx, yy), method='nearest')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.pcolor(interpolated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0131284,  0.0130977,  0.0130165,  0.0130461,  0.0130579,\n",
       "         0.0130705,  0.0130673,  0.013014 ,  0.013067 ,  0.0130673,\n",
       "         0.0130742,  0.0130424,  0.0131063,  0.013041 ,  0.0130354,\n",
       "         0.0130415,  0.0130442,  0.0129011,  0.0129984,  0.0130359,\n",
       "         0.0130761,  0.0130623,  0.0130196,  0.0130593,  0.0130596,\n",
       "         0.0130447,  0.013058 ,  0.0130686,  0.0130626,  0.0130784],\n",
       "       [ 0.0130309,  0.0130342,  0.0130773,  0.013095 ,  0.0130655,\n",
       "         0.0130709,  0.0130834,  0.0130363,  0.0130766,  0.0130537,\n",
       "         0.012883 ,  0.0130093,  0.0130959,  0.0130351,  0.0130685,\n",
       "         0.0130667,  0.0130831,  0.0129268,  0.0130313,  0.0130226,\n",
       "         0.01302  ,  0.0130421,  0.0130307,  0.0130727,  0.0130674,\n",
       "         0.013017 ,  0.013053 ,  0.0130509,  0.0130441,  0.0130853],\n",
       "       [ 0.0130076,  0.0130482,  0.0130506,  0.0130161,  0.0130545,\n",
       "         0.0130134,  0.0130176,  0.0130303,  0.0130606,  0.0130398,\n",
       "         0.0130371,  0.0130427,  0.0130241,  0.0130324,  0.0130324,\n",
       "         0.0130261,  0.0129602,  0.0129288,  0.0129862,  0.0130262,\n",
       "         0.0130061,  0.0130101,  0.0130434,  0.0130382,  0.0130001,\n",
       "         0.0129817,  0.012998 ,  0.0130463,  0.0130678,  0.0130331],\n",
       "       [ 0.0129934,  0.013033 ,  0.0130116,  0.0130222,  0.0130083,\n",
       "         0.0130308,  0.013    ,  0.0130095,  0.0130049,  0.0129833,\n",
       "         0.012997 ,  0.0129974,  0.0129981,  0.0130548,  0.0130251,\n",
       "         0.0130171,  0.0129384,  0.0129166,  0.0130323,  0.013015 ,\n",
       "         0.0129939,  0.0129921,  0.0130225,  0.0130074,  0.0130633,\n",
       "         0.0129911,  0.0129835,  0.0130655,  0.0129995,  0.0129818],\n",
       "       [ 0.0129748,  0.0129416,  0.0129553,  0.0129737,  0.0129825,\n",
       "         0.0130094,  0.0129584,  0.0129495,  0.0129923,  0.0129904,\n",
       "         0.0129665,  0.0129505,  0.0129662,  0.0129914,  0.0129955,\n",
       "         0.0130118,  0.0128261,  0.0129328,  0.0129888,  0.0129225,\n",
       "         0.0129505,  0.012959 ,  0.0129742,  0.0129693,  0.0130199,\n",
       "         0.0129735,  0.0130046,  0.0129428,  0.0129986,  0.0129525],\n",
       "       [ 0.0129746,  0.0129743,  0.0129592,  0.012995 ,  0.0129782,\n",
       "         0.0129573,  0.0129753,  0.0129501,  0.0129708,  0.012984 ,\n",
       "         0.0129706,  0.0129467,  0.0129778,  0.0129432,  0.0129653,\n",
       "         0.012945 ,  0.0128224,  0.0129213,  0.012946 ,  0.012965 ,\n",
       "         0.0129304,  0.0129936,  0.0129768,  0.0129453,  0.0129717,\n",
       "         0.0129581,  0.0129242,  0.0129755,  0.0129609,  0.0129866],\n",
       "       [ 0.0129578,  0.012948 ,  0.0129179,  0.0128749,  0.0129351,\n",
       "         0.012935 ,  0.0129302,  0.0129051,  0.0128886,  0.012919 ,\n",
       "         0.012968 ,  0.0129925,  0.0129548,  0.0128705,  0.0128942,\n",
       "         0.0129102,  0.0127968,  0.0129172,  0.0129444,  0.0129032,\n",
       "         0.0129481,  0.0129385,  0.0129578,  0.0129613,  0.0129207,\n",
       "         0.0129127,  0.0128965,  0.012929 ,  0.0129213,  0.0129048],\n",
       "       [ 0.012873 ,  0.012869 ,  0.0128665,  0.0128723,  0.0128785,\n",
       "         0.0128748,  0.0128946,  0.0129008,  0.0128481,  0.0129099,\n",
       "         0.0128983,  0.0128809,  0.0128964,  0.0128874,  0.0129008,\n",
       "         0.0128325,  0.0127805,  0.0128962,  0.0129294,  0.0128569,\n",
       "         0.0129368,  0.012895 ,  0.0129053,  0.0129215,  0.0129003,\n",
       "         0.0128899,  0.0129159,  0.0128744,  0.0128794,  0.01287  ],\n",
       "       [ 0.0128763,  0.0129107,  0.0128922,  0.0128436,  0.0128826,\n",
       "         0.012865 ,  0.0128973,  0.0128964,  0.0128712,  0.0129035,\n",
       "         0.0128459,  0.0128768,  0.0128641,  0.0128703,  0.0129098,\n",
       "         0.0127678,  0.0128003,  0.0128784,  0.012824 ,  0.012861 ,\n",
       "         0.012845 ,  0.0128296,  0.0128422,  0.0128885,  0.0128645,\n",
       "         0.012848 ,  0.0128508,  0.0128341,  0.0128578,  0.0129282],\n",
       "       [ 0.0127861,  0.0128656,  0.0128201,  0.0128475,  0.0128328,\n",
       "         0.0128438,  0.0128728,  0.0128781,  0.0129004,  0.0128496,\n",
       "         0.0128932,  0.0128689,  0.0128535,  0.0128619,  0.012949 ,\n",
       "         0.0127467,  0.0128048,  0.0128431,  0.0128296,  0.0128422,\n",
       "         0.0128087,  0.012861 ,  0.0128229,  0.0128809,  0.0128761,\n",
       "         0.0129035,  0.0128798,  0.012853 ,  0.0128364,  0.0128212],\n",
       "       [ 0.0128094,  0.0128387,  0.0128222,  0.0128403,  0.0128598,\n",
       "         0.0128292,  0.0128313,  0.0128296,  0.0128334,  0.0128218,\n",
       "         0.0128164,  0.0128492,  0.0128185,  0.0129107,  0.0129379,\n",
       "         0.0126629,  0.0127871,  0.0128034,  0.0128278,  0.0128317,\n",
       "         0.0128162,  0.0128057,  0.0127978,  0.0127934,  0.0128721,\n",
       "         0.0128499,  0.0128489,  0.0128276,  0.0128192,  0.0128189],\n",
       "       [ 0.0127744,  0.0128512,  0.0127768,  0.0128556,  0.0128217,\n",
       "         0.0128101,  0.012824 ,  0.0127886,  0.0128386,  0.0127989,\n",
       "         0.012813 ,  0.0128512,  0.0128426,  0.0128844,  0.0129239,\n",
       "         0.0126378,  0.0127575,  0.0127717,  0.0128386,  0.012851 ,\n",
       "         0.0128161,  0.0128163,  0.0128231,  0.0128284,  0.0128428,\n",
       "         0.0128251,  0.0128054,  0.0128337,  0.0128012,  0.0128119],\n",
       "       [ 0.0127462,  0.0127675,  0.012792 ,  0.0127999,  0.0128061,\n",
       "         0.0128249,  0.012765 ,  0.0128007,  0.0127954,  0.0128128,\n",
       "         0.0128289,  0.0127589,  0.0127622,  0.0129028,  0.0128862,\n",
       "         0.0125541,  0.0127699,  0.012768 ,  0.0127733,  0.012807 ,\n",
       "         0.0127973,  0.0127806,  0.0127708,  0.0127798,  0.012778 ,\n",
       "         0.0128029,  0.0128138,  0.0128263,  0.0128451,  0.0128035],\n",
       "       [ 0.0127712,  0.0127884,  0.0127908,  0.0127682,  0.0127987,\n",
       "         0.012764 ,  0.0127693,  0.0127766,  0.0127717,  0.0127522,\n",
       "         0.0127621,  0.0128117,  0.0128251,  0.0130733,  0.0127873,\n",
       "         0.012526 ,  0.0127438,  0.0127705,  0.0128116,  0.0128179,\n",
       "         0.0127726,  0.0127956,  0.012808 ,  0.0127891,  0.0127647,\n",
       "         0.0127835,  0.0127701,  0.0127585,  0.0127714,  0.0127533],\n",
       "       [ 0.0127432,  0.0127565,  0.0127781,  0.0127623,  0.0127741,\n",
       "         0.0127834,  0.0127932,  0.0127795,  0.0127528,  0.0128059,\n",
       "         0.0128146,  0.0128601,  0.0129065,  0.0133745,  0.012508 ,\n",
       "         0.0123649,  0.0127365,  0.0127994,  0.0127699,  0.012756 ,\n",
       "         0.0127372,  0.0127462,  0.0127844,  0.012749 ,  0.0127309,\n",
       "         0.0127579,  0.0127843,  0.0127697,  0.0127771,  0.012756 ],\n",
       "       [ 0.0127394,  0.0127919,  0.0127431,  0.0128069,  0.0127863,\n",
       "         0.0127809,  0.0128148,  0.0127797,  0.012787 ,  0.0127495,\n",
       "         0.0128193,  0.0128414,  0.0130628,  0.0135273,  0.0118761,\n",
       "         0.0123234,  0.0127298,  0.0127628,  0.0127416,  0.0127279,\n",
       "         0.0127707,  0.0127226,  0.0127802,  0.0127726,  0.0127844,\n",
       "         0.0127818,  0.0127563,  0.0127452,  0.0127505,  0.0127619],\n",
       "       [ 0.0127692,  0.0127826,  0.0127462,  0.0127415,  0.0127315,\n",
       "         0.0127592,  0.0127389,  0.0127452,  0.012768 ,  0.0127559,\n",
       "         0.0127745,  0.0128891,  0.0130868,  0.0129237,  0.0114431,\n",
       "         0.0120937,  0.0127164,  0.0127692,  0.0127389,  0.0127364,\n",
       "         0.0127548,  0.0127466,  0.012732 ,  0.0127499,  0.0127454,\n",
       "         0.0127457,  0.0127524,  0.012757 ,  0.012771 ,  0.0127153],\n",
       "       [ 0.0127258,  0.0127267,  0.012749 ,  0.0127478,  0.0127028,\n",
       "         0.0127227,  0.0127192,  0.0127552,  0.0128814,  0.0127134,\n",
       "         0.0127764,  0.0128114,  0.0127221,  0.0111119,  0.0110629,\n",
       "         0.0122693,  0.0127021,  0.0127546,  0.0129188,  0.01273  ,\n",
       "         0.0127096,  0.0127367,  0.0127365,  0.0127308,  0.0127148,\n",
       "         0.0127415,  0.0126983,  0.0127504,  0.0127651,  0.0127604],\n",
       "       [ 0.0127   ,  0.0127279,  0.012747 ,  0.012723 ,  0.0127325,\n",
       "         0.0127176,  0.0127721,  0.012686 ,  0.0127527,  0.0127047,\n",
       "         0.0126373,  0.0125199,  0.011979 ,  0.0098454,  0.0109596,\n",
       "         0.0122546,  0.0126594,  0.0126761,  0.0127377,  0.0127039,\n",
       "         0.0127197,  0.0126944,  0.0127397,  0.0127154,  0.0127307,\n",
       "         0.0127556,  0.0126868,  0.0126914,  0.0127104,  0.0127107],\n",
       "       [ 0.0126757,  0.0126595,  0.0127273,  0.0127403,  0.0127364,\n",
       "         0.012731 ,  0.0127164,  0.0126458,  0.0126492,  0.0125614,\n",
       "         0.0124817,  0.0123596,  0.0120898,  0.0108086,  0.0112552,\n",
       "         0.0124907,  0.0126871,  0.0127122,  0.0126958,  0.0126909,\n",
       "         0.012679 ,  0.0127036,  0.012686 ,  0.0126925,  0.0127109,\n",
       "         0.0126715,  0.0126974,  0.0127313,  0.0126974,  0.0127452],\n",
       "       [ 0.0126883,  0.0126821,  0.0126818,  0.0126906,  0.0126855,\n",
       "         0.012664 ,  0.0125819,  0.0125603,  0.0125417,  0.0125733,\n",
       "         0.0125148,  0.0124795,  0.0123818,  0.0121939,  0.0122579,\n",
       "         0.0125497,  0.0126598,  0.0126595,  0.0127108,  0.012728 ,\n",
       "         0.0126737,  0.0127573,  0.0127397,  0.0126723,  0.012695 ,\n",
       "         0.0126948,  0.012728 ,  0.0127294,  0.0127315,  0.0127285],\n",
       "       [ 0.012672 ,  0.0127201,  0.0126486,  0.0126862,  0.0126467,\n",
       "         0.012617 ,  0.0126054,  0.0126428,  0.0125696,  0.0125952,\n",
       "         0.01262  ,  0.0125751,  0.0126193,  0.0125803,  0.0125319,\n",
       "         0.0126009,  0.0126755,  0.0127044,  0.0126718,  0.0127597,\n",
       "         0.0127854,  0.012795 ,  0.0127581,  0.0126967,  0.012729 ,\n",
       "         0.0127285,  0.0127344,  0.012738 ,  0.0126969,  0.0127006],\n",
       "       [ 0.0126774,  0.0126461,  0.0126698,  0.0126384,  0.0126549,\n",
       "         0.0126154,  0.0126665,  0.0126583,  0.0126656,  0.0126576,\n",
       "         0.0126544,  0.01263  ,  0.0126647,  0.0126335,  0.0126579,\n",
       "         0.0126356,  0.0126904,  0.0126505,  0.01269  ,  0.0128451,\n",
       "         0.0130313,  0.0130487,  0.0128942,  0.0127668,  0.0126916,\n",
       "         0.0126795,  0.0126395,  0.0126702,  0.0126863,  0.0126849],\n",
       "       [ 0.0126548,  0.0126481,  0.0126556,  0.0126123,  0.0126388,\n",
       "         0.0126363,  0.0126504,  0.0126023,  0.0126453,  0.0126189,\n",
       "         0.012692 ,  0.0126595,  0.0126454,  0.0126326,  0.0126505,\n",
       "         0.0126706,  0.0126512,  0.0126797,  0.012742 ,  0.0132744,\n",
       "         0.0138622,  0.0132521,  0.012746 ,  0.0126428,  0.0126707,\n",
       "         0.0126686,  0.0126402,  0.012616 ,  0.0126442,  0.0126326],\n",
       "       [ 0.0126089,  0.01259  ,  0.0126425,  0.0126643,  0.0126767,\n",
       "         0.0126341,  0.0125789,  0.0126208,  0.0126499,  0.0126395,\n",
       "         0.0126316,  0.0126121,  0.0126282,  0.0126447,  0.0126177,\n",
       "         0.0126481,  0.0126207,  0.0126477,  0.0128922,  0.0140598,\n",
       "         0.0141474,  0.0128116,  0.012608 ,  0.0125921,  0.0126271,\n",
       "         0.0125978,  0.0126125,  0.0126038,  0.0126141,  0.01259  ],\n",
       "       [ 0.0125901,  0.0126269,  0.0125943,  0.0125814,  0.0126225,\n",
       "         0.0125358,  0.0126492,  0.0126429,  0.0126452,  0.0126081,\n",
       "         0.0126306,  0.0126111,  0.0125888,  0.0126043,  0.0126268,\n",
       "         0.0126241,  0.0126176,  0.0126245,  0.0129793,  0.0142107,\n",
       "         0.0132262,  0.01227  ,  0.0123868,  0.0124762,  0.012533 ,\n",
       "         0.0125539,  0.0126125,  0.0125679,  0.012592 ,  0.0126236],\n",
       "       [ 0.0125978,  0.0126209,  0.0125658,  0.0125672,  0.0125746,\n",
       "         0.0126241,  0.0125751,  0.0125874,  0.0125597,  0.0126076,\n",
       "         0.0125964,  0.0125758,  0.012612 ,  0.0126099,  0.0125772,\n",
       "         0.0125964,  0.012638 ,  0.012639 ,  0.0127507,  0.0137987,\n",
       "         0.0099739,  0.0095994,  0.011493 ,  0.0121667,  0.0124389,\n",
       "         0.01249  ,  0.0125914,  0.0125806,  0.012599 ,  0.0126086],\n",
       "       [ 0.0125916,  0.0125744,  0.0125915,  0.0125925,  0.0125981,\n",
       "         0.0126034,  0.0126234,  0.0125686,  0.0125795,  0.0125592,\n",
       "         0.0126159,  0.0125751,  0.0125922,  0.0125618,  0.0125544,\n",
       "         0.0125711,  0.0126232,  0.0126301,  0.0128014,  0.0131585,\n",
       "         0.0094514,  0.0083262,  0.0113995,  0.0123537,  0.0124601,\n",
       "         0.0125323,  0.0125906,  0.0125779,  0.0126045,  0.012626 ],\n",
       "       [ 0.0125507,  0.0125516,  0.0125474,  0.0125235,  0.0125476,\n",
       "         0.0125704,  0.01254  ,  0.0125493,  0.0125579,  0.0126023,\n",
       "         0.0125451,  0.0125611,  0.0126204,  0.0126074,  0.012549 ,\n",
       "         0.0125672,  0.0125964,  0.0126261,  0.0127821,  0.0126227,\n",
       "         0.0103234,  0.0105495,  0.0121601,  0.0123994,  0.0125031,\n",
       "         0.0125295,  0.0125753,  0.0125718,  0.0126178,  0.0125499],\n",
       "       [ 0.012524 ,  0.0125383,  0.0125288,  0.0125514,  0.0125353,\n",
       "         0.0125161,  0.0125506,  0.0125455,  0.0125351,  0.012583 ,\n",
       "         0.0125181,  0.0125028,  0.0125233,  0.0125388,  0.0125277,\n",
       "         0.0125225,  0.0125453,  0.0125704,  0.0127861,  0.0122885,\n",
       "         0.011116 ,  0.0115803,  0.0123817,  0.0124644,  0.0125451,\n",
       "         0.0125153,  0.0125611,  0.0125393,  0.0125325,  0.0125569]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "export_savedmodel() missing 1 required positional argument: 'serving_input_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-675d41a2915b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdd_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_savedmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/dd_classifier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: export_savedmodel() missing 1 required positional argument: 'serving_input_fn'"
     ]
    }
   ],
   "source": [
    "dd_classifier.export_savedmodel('/tmp/dd_classifier',serving_input_fn=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
